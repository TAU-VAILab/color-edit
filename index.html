<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
    <link href='https://fonts.googleapis.com/css?family=Noto Sans' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Indie Flower' rel='stylesheet'>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="google-site-verification" content="xdvJxvo39Ei0nahgmgXGp9DCslFea8wH789x6mmAY-A" />
    
    <meta property="og:site_name" content="color-in-context" />
    <meta property="og:type" content="video.other" />
    <meta property="og:title" content="Can Roses be Red and Violets Blue?
Exploring Color Perception in Text-to-Image Models" />
    <meta property="og:description" content="We use voxel grids and 2D diffusion to make local and global textual edits to 3D shapes" />
    <meta property="og:url" content="https://tau-vailab.github.io/color-in-context/" />
    <meta property="og:image" content="https://tau-vailab.github.io/color-in-context/images/voxe_sample.png" /> -->

    <meta property="article:publisher" content="https://tau-vailab.github.io/Vox-E/" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Vox-E: Text-guided Voxel Editing of 3D Objecjts" />
    <meta name="twitter:description" content="We use voxel grids and 2D diffusion to make local and global textual edits to 3D shapes" />
    <meta name="twitter:url" content="https://tau-vailab.github.io/Vox-E/" />
    <meta name="twitter:image" content="https://tau-vailab.github.io/Vox-E/images/voxe_sample.png" />

    <title>Can Roses be Red and Violets Blue?
        Exploring Color Perception in Text-to-Image Models</title>
   <link rel="icon" href="../pics/wis_logo.jpg">
    <link rel="icon" href="images/browser_icon.png">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@9/swiper-bundle.min.css">
    <link href="style.css" rel="stylesheet" type="text/css">
</head>
<body>
<div class="page-container" >
    <script src="https://cdn.jsdelivr.net/npm/swiper@9/swiper-bundle.min.js"></script>

    <!-- title -->
    <h1 class="ourh1" align="center">Can Roses be Red and Violets Blue?</h1>
    <h2 class="ourh2" align="center">Exploring Color Perception in Text-to-Image Models</h2>

    <!-- authors and affiliations -->
    <section class="authors_block">
        <div class="authors" align="center">
            <span class="author-block"><a href="https://shay5510.github.io/" target="_blank">Shay Shomer Chai</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://lilydaytoy.github.io/" target="_blank">Wenxuan Peng</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://www.cs.cornell.edu/~bharathh/" target="_blank">Bharath Hariharan</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://www.elor.sites.tau.ac.il/" target="_blank">Hadar Averbuch-Elor</a><sup>1</sup></span>
        </div>

        <div class="affiliations" align="center">
            <span class="author-block"><sup>1</sup>Tel Aviv University, </span>
            <span class="author-block"><sup>2</sup>Cornell University</span>
        </div>
    </section>
	
	<!-- authors and affiliations -->
	<!-- <section class="conference">
        <div class="conference" align="center">
            <span class="conference-block">TBD 2025</span>
        </div>
    </section> -->
	
    <!-- link buttons -->
    <div class="column has-text-centered">
        <div class="publication-links" align="center">
          
          <!-- arxiv link -->
          <span class="link-block">
            <a href="https://arxiv.org/abs/2303.12048/" class="paper-link" style="display: inline-block">
                <button class="button">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </button>
            </a>
          </span>
          
          <!-- Github Link. -->
          <span class="link-block">
            <a href="https://github.com/TAU-VAILab/color-in-context" style="display: inline-block">
                <button class="button">
                    <span class="icon">
                        <i class="fa fa-github"></i>
                    </span>
                    <span>Code</span>
                  </button>
            </a>
          </span>

          <!-- Supp Link. 
          <span class="link-block">
            <a href="supp/index.html" style="display: inline-block">
                <button class="button">
                    <span class="icon">
                        <i class="fa fa-plus-square"></i>
                    </span>
                    <span>Supplementary Material</span>
                  </button>
            </a>
          </span>-->
        </div>
    </div>

    <!-- tizer -->
    <section class="tizer-section" width="100%">
        <div class="tizer-container">
            <hr>
            <!-- <h2 align="center">How does it work?</h2> -->
            <div class="im_container has-text-justified" width="90%" align="center">
                <img align="center" src="images/tizer.png" alt="Overview" width="100%">
            </div>            
            <p class="has-text-justified">
            </p>
        <div class="attn-grid-vid-container">
            
        </div>
        </div>
    </section>

    <!-- intro -->
    <section class="into-paragraph-section" width="100%">
        <div class="intro-container has-text-justified">
            <div class="intro-paragraph">
                <p>
                    Text-to-image generation methods depicted above face challenges in accurately
                    representing the semantics of multi-subject prompts, such as: "a <span class="prompt_title_local_r">red</span> colored rose and a <span class="prompt_title_local_b">blue</span> colored lily".
                    In this work, we introduce a new benchmark for exploring this problem (also commonly referred to as multi-attribute leakage), 
                    and propose an image editing technique, tailored for mitigating it in the case of multiple colors.
                </p>
            </div>
        </div>
    </section>

    <!-- abstract -->
    <section class="abstract-section" width="100%">
        <div class="abstract-container has-text-justified">
            <hr>
            <h2 align="center">Abstract</h2>
            <p class="has-text-justified">
                Text-to-image generation has recently seen remarkable success, granting users with the ability to create high-quality images
                through the use of text. However, contemporary methods face challenges in capturing the precise semantics conveyed by complex
                multi-object prompts. Consequently, many works have sought to mitigate such semantic misalignments, typically via inference-time
                schemes that modify the attention layers of the denoising networks. However, prior work has mostly utilized coarse metrics, 
                such as the cosine similarity between text and image CLIP embeddings, visual question-answering capabilities of vision-and-language 
                models, or human evaluations, which are challenging to conduct on a larger-scale. In this work, we perform a case study on colors---a fundamental attribute 
                commonly associated with objects in text prompts, which offer a rich test bed for rigorous evaluation. Our analysis reveals that pretrained models struggle 
                at producing images that adhere to multiple color attributes, significantly more than when provided with a single color specification, while inference-time 
                techniques cannot robustly resolve these semantic misalignments. Accordingly, we introduce a dedicated image editing technique, mitigating the issue of 
                multi-object semantic alignment for prompts containing multiple colors. We demonstrate that our approach significantly boosts performance over a 
                wide range of metrics, considering images generated by various text-to-image diffusion-based techniques. We will make our code, benchmark and 
                evaluation protocol publicly available.
            </p>
            <!-- <div class="swipe_vid_container" align="center" width="100%">
                <video loop autoplay muted width="60%"  class="swipe_video">
                    <source src=".\videos\swipe_mid_qual.mp4" type="video/mp4">
                </video>
            </div> -->
        </div>
    </section>

    <!-- method -->
    <section class="method-section" width="100%">
        <div class="method-container">
            <hr>
            <h2 align="center">The ColorPrompts Benchmark</h2>         
            <p class="has-text-justified">
                In spite of advances in image generation, it remains a question whether current state-of-the-art models can adhere to
                object attributes that are specified in the prompt. This is especially true when the prompt asks for multiple objects.
                For example, when asked to generate an image of “a red colored rose and a blue colored lily”, we find that many current 
                techniques produce a whiter lily than expected.
                To evaluate color fidelity with complex multi-object prompts in text-to-image models, we build our benchmark by creating pairs
                of colors for prompts structured as “a {color1} colored {object1} and a {color2} colored {object2}.” A key factor here is the
                perceptual similarity of the colors. On the one hand, if the two colors are similar, then the model may still succeed at a reasonable
                generation even if it mixes the colors. On the other hand, similar perceptual colors may pose a challenge for methods which try to ensure 
                that the generated objects have distinct attributes. As such, we classify the pairs into two types: close and distant, based on their perceptual 
                similarity in the CIELAB color space. Close colors are those that appear visually similar to the human eye, low LAB distance (e.g., <span class="skyblue">SkyBlue</span> vs. <span class="lightcyan">LightCyan</span>), 
                while distant colors are distinctly different, high LAB distance (e.g., <span class="skyblue">SkyBlue</span> vs. <span class="hotpink">HotPink</span>).
            </p>
            <div class="im_container has-text-justified" width="90%" align="center">
                <img align="center" src="images/close_distant2.png" alt="Overview" style="display: block; margin: 0 auto; width: 70%;">
            </div> 
        <div class="attn-grid-vid-container">
            
        </div>
        </div>
    </section>


    <!-- method -->
    <section class="method-section" width="100%">
        <div class="method-container">
            <hr>
            <h2 align="center">ColorEdit - Inference-Time Color Editing</h2>           
            <p class="has-text-justified">
                We desire to correct this problem so as to allow users to precisely control the color 
                of the generated objects. For the sake of generality and to allow our approach to be 
                relevant for future generative models as well, we frame our task as an editing task: 
                the input to our system is a prompt P with color attributes (e.g., “A teal cup and a pink spoon”) 
                and a source image Is with the right objects but potentially the wrong colors. This also allows us 
                to edit real images instead of generated ones. To further enable precise control, we assume that the 
                colors come with specified RGB values (e.g., “teal” = (0,128,128) ). Our goal is to edit the image Is 
                so that it matches the color specification as closely as possible, while preserving its overall appearance 
                and structure and still producing a high quality image.
                We introduce an inference-time approach that utilizes attention-based diffusion models for editing existing images to 
                match color specifications. In particular, we propose two objectives for guiding our optimization scheme: an attention loss 
                for binding the colors to the right object and a color loss that forces objects to have the right color. 
                A key insight enabling our proposed attention loss is that a simplified color-less reference text prompt can provide supervision for 
                grounding noisy attention maps. Inspired by the psychological Stroop effect which suggests that incongruent stimuli such as color words 
                printed in differing colors (e.g. “blue” printed in red) have a stronger interference effect, our loss is built on the premise that misalignments 
                in attribute binding leads to errors in the attention maps. Hence, we propose a loss that encourages the cross-attention maps of the full prompt 
                to be more congruent to the simplified reference prompt, thus allowing for binding the colors to the correct image regions.
            </p>
            <div class="im_container has-text-justified" width="90%" align="center">
                <img align="center" src="images/overview_fg.png" alt="Overview" style="display: block; margin: 0 auto; width: 80%;">
            </div> 
            <p class="has-text-justified">
                Given an input image and a target prompt P containing multiple color attributes, we present an approach for editing
                the image to match the color specification while preserving all other attributes. Our final edited result is illustrated 
                on the bottom right. Two objectives guide our inference-time optimization procedure: an  attention loss that binds colors 
                to the right object and a color loss that gets the right colors; for simplicity we only demonstrate these for the ``pink spoon" 
                in the visualization above.
            </p>
        <div class="attn-grid-vid-container">
            
        </div>
        </div>
    </section>

    <!-- BibTex -->
    <!-- <section class="bib-section" width="100%">
        <div class="bib-container">
            <hr>
            <h2 align="center">BibTeX</h2>
            <div class="code-container">
                <code>
                  @misc{sella2023voxe, <br>
                    &emsp;  title={Vox-E: Text-guided Voxel Editing of 3D Objects}, <br>
                    &emsp;  author={Etai Sella and Gal Fiebelman and Peter Hedman and Hadar Averbuch-Elor}, <br>
                    &emsp;  year={2023}, <br>
                    &emsp;  eprint={2303.12048}, <br>
                    &emsp;  archivePrefix={arXiv}, <br>
                    &emsp;  primaryClass={cs.CV} <br>
                  }
                </code>
            </div> 
        </div>
    </section> -->

    <!-- Acknowledgements -->
    <!-- <section class="ack-section" width="100%"> -->
    <!--     <div class="ack-container"> -->
    <!--         <hr> -->
    <!--         <h2 align="center">Acknowledgements</h2> -->
    <!--         <p> -->
    <!--             We would like to acknowledge such and such for this and that. -->
    <!--         </p> -->
    <!--     </div> -->
    <!-- </section> -->

  <p><br>
  </p>
  <p>&nbsp;</p>
  <p>&nbsp;</p>
  <p>&nbsp;</p>

</div>
<script>
const swiper = new Swiper('.swiper', {
    autoplay: {
    delay: 4000,
    },
    // Optional parameters
    speed: 1000,
    loop: true,
  
    // If we need pagination
    pagination: {
      el: '.swiper-pagination',
    },
  
    // Navigation arrows
    navigation: {
      nextEl: '.swiper-button-next',
      prevEl: '.swiper-button-prev',
    },
  });
</script>

</body></html>